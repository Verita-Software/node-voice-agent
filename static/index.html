<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepgram Voice Agent</title>
</head>
<body>
    <h1>Intelidoc Ai agent</h1>
    
    <script>
        let socket;
        let mediaStream;
        let audioContext;
        let processor;
        let isConnected = false;
        let audioQueue = []; // Queue for managing incoming audio chunks
        let isPlaying = false; // Flag to track if we're currently playing audio
        let selectedDeviceId;

        async function init() {
            try {
                // Create audio context early
                audioContext = new AudioContext({
                    sampleRate: 24000 // Match the highest sample rate we'll use
                });

                // Get microphone permission with specific constraints
                const constraints = {
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        volume: 1.0,
                        echoCancellation: true,
                        noiseSuppression: false,
                        latency: 0,
                        },
                };
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Connect to WebSocket server
                socket = new WebSocket('ws://localhost:3000');

                socket.onopen = () => {
                    isConnected = true;
                    startStreaming();
                };

                socket.onmessage = async (event) => {
                    if (event.data instanceof Blob) {
                        try {
                            const arrayBuffer = await event.data.arrayBuffer();
                            const audioData = new Int16Array(arrayBuffer);
                            // Add to queue instead of playing immediately
                            audioQueue.push(audioData);
                            if (!isPlaying) {
                                playNextInQueue();
                            }
                        } catch (error) {
                            console.error('Error processing audio response:', error);
                        }
                    }
                };

                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    isConnected = false;
                };

                socket.onclose = () => {
                    isConnected = false;
                    stopStreaming();
                };
            } catch (error) {
                console.error('Error initializing:', error);
            }
        }

        async function setupAudioProcessing() {
            const source = audioContext.createMediaStreamSource(mediaStream);

            // Gain control
            const gainNode = audioContext.createGain();

            // Analyzer for volume monitoring
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 1024;

            // Worklet processor for better performance
            const processor = audioContext.createScriptProcessor(2048, 1, 1);

            // Connect the chain
            source
                .connect(gainNode)
                .connect(analyser)
                .connect(processor)
                .connect(audioContext.destination);

            return { gainNode, analyser, processor };
        }

        function startStreaming() {
            if (!mediaStream || !isConnected) return;

            try {
                const source = audioContext.createMediaStreamSource(mediaStream);

                // Create a worklet for better audio processing
                const bufferSize = 2048;
                processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                let lastSendTime = 0;
                const sendInterval = 50; // Send every 50ms for better responsiveness

                processor.onaudioprocess = (e) => {
                    const now = Date.now();
                    if (socket?.readyState === WebSocket.OPEN && now - lastSendTime >= sendInterval) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        
                        // Apply some gain to boost input signal
                        const amplifiedData = new Float32Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            amplifiedData[i] = Math.max(-1, Math.min(1, inputData[i] * 2.0));
                        }
                        
                        const pcmData = convertFloatToPcm(amplifiedData);
                        socket.send(pcmData.buffer);
                        lastSendTime = now;
                    }
                };
            } catch (error) {
                console.error('Error starting audio stream:', error);
            }
        }

        function convertFloatToPcm(floatData) {
            const pcmData = new Int16Array(floatData.length);
            for (let i = 0; i < floatData.length; i++) {
                const s = Math.max(-1, Math.min(1, floatData[i]));
                pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcmData;
        }

        async function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioData = audioQueue.shift();

            try {
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Create buffer and resample if needed
                const targetSampleRate = audioContext.sampleRate;
                const sourceSampleRate = 24000;
                let finalAudioData = audioData;
                
                // Simple resampling if context sample rate differs
                if (targetSampleRate !== sourceSampleRate) {
                    const ratio = targetSampleRate / sourceSampleRate;
                    const newLength = Math.ceil(audioData.length * ratio);
                    const resampledData = new Int16Array(newLength);
                    
                    for (let i = 0; i < newLength; i++) {
                        const sourceIndex = Math.floor(i / ratio);
                        resampledData[i] = audioData[Math.min(sourceIndex, audioData.length - 1)];
                    }
                    finalAudioData = resampledData;
                }
                
                const buffer = audioContext.createBuffer(1, finalAudioData.length, targetSampleRate);
                const channelData = buffer.getChannelData(0);

                // Convert Int16 to Float32 with proper scaling
                for (let i = 0; i < finalAudioData.length; i++) {
                    // Normalize to [-1, 1] range with better precision
                    channelData[i] = finalAudioData[i] >= 0 ? finalAudioData[i] / 32767 : finalAudioData[i] / 32768;
                }

                // Create and configure source
                const source = audioContext.createBufferSource();
                source.buffer = buffer;

                // Create a gain node for volume control
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 2.0; // Increase volume to fix low audio

                // Connect nodes
                source.connect(gainNode);
                gainNode.connect(audioContext.destination);

                // Handle playback completion
                source.onended = () => {
                    playNextInQueue(); // Play next chunk when current one ends
                };

                // Start playback
                source.start(0);
            } catch (error) {
                console.error('Error playing audio:', error);
                isPlaying = false;
                playNextInQueue(); // Try next chunk if current fails
            }
        }

        function stopStreaming() {
            audioQueue = []; // Clear audio queue
            isPlaying = false;
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            isConnected = false;
        }

        function initializeVolumeMeter(analyser) {
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function updateMeter() {
                analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                const volume = Math.min(100, average * 2);
                // Update UI with volume level
                requestAnimationFrame(updateMeter);
            }

            updateMeter();
        }

        async function getAudioDevices() {
            const devices = await navigator.mediaDevices.enumerateDevices();
            return devices.filter(device => device.kind === 'audioinput');
        }

        // Initialize when the page loads
        window.onload = init;

        // Clean up when the page is closed
        window.onbeforeunload = () => {
            stopStreaming();
            if (socket) {
                socket.close();
            }
        };
    </script>
</body>
</html>